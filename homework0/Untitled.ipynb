{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e93d6cc-aac2-4487-8854-04a39ec48ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/envs/sau24s/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d2ec5d-e0dd-45bf-9e0d-6988739c9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/envs/sau24s/lib/python3.8/site-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54aee0b0-68e3-4cbd-8f04-ef3a44613487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24afe287-2288-4071-9145-2773881d9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ce40a6-a9ef-45c2-b9a4-7259cbac0b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2008-12-15-2009-01-21 saved successfully.\n",
      "Data for 2009-12-15-2010-01-21 saved successfully.\n",
      "Data for 2010-12-15-2011-01-21 saved successfully.\n",
      "Data for 2011-12-15-2012-01-21 saved successfully.\n",
      "Data for 2012-12-15-2013-01-21 saved successfully.\n",
      "Error fetching data: 502 Server Error: Proxy Error for url: https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:80249&units=standard&startdate=2013-12-15&enddate=2014-01-21&limit=1000\n",
      "Data for 2014-12-15-2015-01-21 saved successfully.\n",
      "Data for 2015-12-15-2016-01-21 saved successfully.\n",
      "Data for 2016-12-15-2017-01-21 saved successfully.\n",
      "Data for 2017-12-15-2018-01-21 saved successfully.\n",
      "Data for 2018-12-15-2019-01-21 saved successfully.\n",
      "Data for 2019-12-15-2020-01-21 saved successfully.\n",
      "Data for 2020-12-15-2021-01-21 saved successfully.\n",
      "Data for 2021-12-15-2022-01-21 saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "def fetch_data(start_date, end_date, file_name):\n",
    "    token = 'peUoGReGomLyRqAedzPNFbpUtrtznRun'\n",
    "    url = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:80249&units=standard&startdate={start_date}&enddate={end_date}&limit=1000\"\n",
    "    headers = {\"token\": token}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        with open(os.path.join(data_dir, file_name), 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        print(f\"Data for {start_date}-{end_date} saved successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "\n",
    "# Define the date ranges\n",
    "date_ranges = [\n",
    "    (\"2008-12-15\", \"2009-01-21\"),\n",
    "    (\"2009-12-15\", \"2010-01-21\"),\n",
    "    (\"2010-12-15\", \"2011-01-21\"),\n",
    "    (\"2011-12-15\", \"2012-01-21\"),\n",
    "    (\"2012-12-15\", \"2013-01-21\"),\n",
    "    (\"2013-12-15\", \"2014-01-21\"),\n",
    "    (\"2014-12-15\", \"2015-01-21\"),\n",
    "    (\"2015-12-15\", \"2016-01-21\"),\n",
    "    (\"2016-12-15\", \"2017-01-21\"),\n",
    "    (\"2017-12-15\", \"2018-01-21\"),\n",
    "    (\"2018-12-15\", \"2019-01-21\"),\n",
    "    (\"2019-12-15\", \"2020-01-21\"),\n",
    "    (\"2020-12-15\", \"2021-01-21\"),\n",
    "    (\"2021-12-15\", \"2022-01-21\")\n",
    "]\n",
    "\n",
    "# Fetch data for each date range and save it to a JSON file\n",
    "for i, (start_date, end_date) in enumerate(date_ranges, 1):\n",
    "    file_name = f'winter_{start_date[:4]}-{end_date[:4]}.json'\n",
    "    fetch_data(start_date, end_date, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974f58bb-5feb-4288-b870-40f9b2c3048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/envs/sau24s/lib/python3.8/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df93ced-60d8-4111-a404-dae5d6f4307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2008-12-15-2009-01-21 saved successfully.\n",
      "Data for 2009-12-15-2010-01-21 saved successfully.\n",
      "Data for 2010-12-15-2011-01-21 saved successfully.\n",
      "Data for 2011-12-15-2012-01-21 saved successfully.\n",
      "Data for 2012-12-15-2013-01-21 saved successfully.\n",
      "Data for 2013-12-15-2014-01-21 saved successfully.\n",
      "Data for 2014-12-15-2015-01-21 saved successfully.\n",
      "Data for 2015-12-15-2016-01-21 saved successfully.\n",
      "Data for 2016-12-15-2017-01-21 saved successfully.\n",
      "Data for 2017-12-15-2018-01-21 saved successfully.\n",
      "Data for 2018-12-15-2019-01-21 saved successfully.\n",
      "Data for 2019-12-15-2020-01-21 saved successfully.\n",
      "Data for 2020-12-15-2021-01-21 saved successfully.\n",
      "Data for 2021-12-15-2022-01-21 saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "def fetch_data(start_date, end_date, file_name):\n",
    "    token = 'peUoGReGomLyRqAedzPNFbpUtrtznRun'\n",
    "    url = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:80249&units=standard&startdate={start_date}&enddate={end_date}&limit=1000\"\n",
    "    headers = {\"token\": token}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        with open(os.path.join(data_dir, file_name), 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        print(f\"Data for {start_date}-{end_date} saved successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "\n",
    "# Define the date ranges\n",
    "date_ranges = [\n",
    "    (\"2008-12-15\", \"2009-01-21\"),\n",
    "    (\"2009-12-15\", \"2010-01-21\"),\n",
    "    (\"2010-12-15\", \"2011-01-21\"),\n",
    "    (\"2011-12-15\", \"2012-01-21\"),\n",
    "    (\"2012-12-15\", \"2013-01-21\"),\n",
    "    (\"2013-12-15\", \"2014-01-21\"),\n",
    "    (\"2014-12-15\", \"2015-01-21\"),\n",
    "    (\"2015-12-15\", \"2016-01-21\"),\n",
    "    (\"2016-12-15\", \"2017-01-21\"),\n",
    "    (\"2017-12-15\", \"2018-01-21\"),\n",
    "    (\"2018-12-15\", \"2019-01-21\"),\n",
    "    (\"2019-12-15\", \"2020-01-21\"),\n",
    "    (\"2020-12-15\", \"2021-01-21\"),\n",
    "    (\"2021-12-15\", \"2022-01-21\")\n",
    "]\n",
    "\n",
    "# Fetch data for each date range and save it to a JSON file\n",
    "for i, (start_date, end_date) in enumerate(date_ranges, 1):\n",
    "    file_name = f'winter_{start_date[:4]}-{end_date[:4]}.json'\n",
    "    try:\n",
    "        fetch_data(start_date, end_date, file_name)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "        print(f\"Data for {start_date}-{end_date} could not be fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a24c465-44f2-49dd-a71f-2fc8c768db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Extract required information and calculate TAVG\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df['TMAX'] = combined_df['value'][combined_df['datatype'] == 'TMAX'].reset_index(drop=True)\n",
    "combined_df['TMIN'] = combined_df['value'][combined_df['datatype'] == 'TMIN'].reset_index(drop=True)\n",
    "combined_df = combined_df.groupby('DATE').agg({'TMAX': 'max', 'TMIN': 'min'}).reset_index()\n",
    "combined_df['TAVG'] = (combined_df['TMAX'] + combined_df['TMIN']) / 2\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c257515-a532-447c-ba2c-2d697fd5e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2b7194-f1a7-4ec7-b223-0b437b1abab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Sort DataFrame by DATE\n",
    "merged_df.sort_values(by='DATE', inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a87c54-05fe-4227-877f-4cfc480ad46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load and preprocess data from a JSON file\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        df['DATE'] = pd.to_datetime(df['date'])\n",
    "        df['TAVG'] = df['value']  # Assuming 'TAVG' is already in the required unit/format\n",
    "    return df[['DATE', 'TAVG']]\n",
    "\n",
    "# Combine all TAVG data into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "tavg_dfs = []\n",
    "\n",
    "for file in json_files:\n",
    "    tavg_df = load_json_file(os.path.join(data_dir, file))\n",
    "    tavg_dfs.append(tavg_df)\n",
    "\n",
    "combined_tavg_df = pd.concat(tavg_dfs, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_tavg_df = combined_tavg_df[(combined_tavg_df['DATE'] >= start_date) & (combined_tavg_df['DATE'] <= end_date)]\n",
    "\n",
    "# Create a DataFrame with rows as dates and columns as years, and fill it with TAVG values\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "years = pd.Series(date_range.year).unique()\n",
    "year_columns = [f'{year}-{year+1}' for year in years]\n",
    "date_index = [date.strftime('%m-%d') for date in date_range]\n",
    "\n",
    "compiled_data = pd.DataFrame(index=date_index, columns=year_columns)\n",
    "\n",
    "for year in years:\n",
    "    year_start = pd.to_datetime(f'{year}-12-15')\n",
    "    year_end = pd.to_datetime(f'{year+1}-01-21')\n",
    "    year_data = combined_tavg_df[(combined_tavg_df['DATE'] >= year_start) & (combined_tavg_df['DATE'] <= year_end)]\n",
    "    for date in date_index:\n",
    "        avg_tavg = year_data[year_data['DATE'].dt.strftime('%m-%d') == date]['TAVG'].mean()\n",
    "        compiled_data.at[date, f'{year}-{year+1}'] = round(avg_tavg, 2)\n",
    "\n",
    "# Export compiled data to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "compiled_data.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d1089e0-f160-42bf-b0d6-6a5640ff0b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "629e5148-7c08-4140-a7ee-a154819288b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Sort DataFrame by DATE\n",
    "merged_df.sort_values(by='DATE', inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6239b0a-978b-4a2d-8fd0-7a91689a58f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load JSON data from file\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "# Function to filter data within date range and calculate TAVG\n",
    "def process_data(json_data, start_date, end_date):\n",
    "    df = pd.DataFrame(json_data['results'])\n",
    "    df['DATE'] = pd.to_datetime(df['date'])\n",
    "    df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
    "    tmax = df[df['datatype'] == 'TMAX']['value']\n",
    "    tmin = df[df['datatype'] == 'TMIN']['value']\n",
    "    df['TAVG'] = (tmax + tmin) / 2\n",
    "    return df[['DATE', 'TAVG']]\n",
    "\n",
    "# Define date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    json_data = load_json_file(os.path.join(data_dir, file))\n",
    "    df = process_data(json_data, start_date, end_date)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Create empty DataFrame to store compiled data\n",
    "compiled_df = pd.DataFrame(columns=['DATE'])\n",
    "\n",
    "# Add rows with dates to the DataFrame\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "compiled_df['DATE'] = date_range.strftime('%m-%d')\n",
    "\n",
    "# Add columns for each year\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    compiled_df[year] = ''\n",
    "\n",
    "# Fill in TAVG values into the DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    for index, row in df.iterrows():\n",
    "        date_index = date_range.get_loc(row['DATE'].strftime('%Y-%m-%d'))\n",
    "        compiled_df.at[date_index, year] = row['TAVG']\n",
    "\n",
    "# Export compiled DataFrame to CSV\n",
    "output_file = 'all_data_avg.csv'\n",
    "compiled_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56c21726-36ab-4ac8-a64f-64ed74a19693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load JSON data from file\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "# Function to filter data within date range and calculate TAVG\n",
    "def process_data(json_data, start_date, end_date):\n",
    "    df = pd.DataFrame(json_data['results'])\n",
    "    df['DATE'] = pd.to_datetime(df['date'])\n",
    "    df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
    "    tmax = df[df['datatype'] == 'TMAX']['value']\n",
    "    tmin = df[df['datatype'] == 'TMIN']['value']\n",
    "    df['TAVG'] = (tmax + tmin) / 2\n",
    "    return df[['DATE', 'TAVG']]\n",
    "\n",
    "# Define date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    json_data = load_json_file(os.path.join(data_dir, file))\n",
    "    df = process_data(json_data, start_date, end_date)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Create empty DataFrame to store compiled data\n",
    "compiled_df = pd.DataFrame(columns=['DATE'])\n",
    "\n",
    "# Add rows with dates to the DataFrame\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "compiled_df['DATE'] = date_range.strftime('%m-%d')\n",
    "\n",
    "# Add columns for each year\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    compiled_df[year] = ''\n",
    "\n",
    "# Fill in TAVG values into the DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    for index, row in df.iterrows():\n",
    "        date_index = date_range.get_loc(row['DATE'].strftime('%Y-%m-%d'))\n",
    "        compiled_df.at[date_index, year] = row['TAVG']\n",
    "\n",
    "# Export compiled DataFrame to CSV in the 'data/' folder\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "compiled_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30b2642a-3c2c-43a3-a06b-377406240859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load JSON data from file\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "# Function to filter data within date range and calculate TAVG\n",
    "def process_data(json_data, start_date, end_date):\n",
    "    df = pd.DataFrame(json_data['results'])\n",
    "    df['DATE'] = pd.to_datetime(df['date'])\n",
    "    df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
    "    tmax = df[df['datatype'] == 'TMAX']['value']\n",
    "    tmin = df[df['datatype'] == 'TMIN']['value']\n",
    "    df['TAVG'] = (tmax + tmin) / 2\n",
    "    return df[['DATE', 'TAVG']]\n",
    "\n",
    "# Define date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    json_data = load_json_file(os.path.join(data_dir, file))\n",
    "    df = process_data(json_data, start_date, end_date)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Create empty DataFrame to store compiled data\n",
    "compiled_df = pd.DataFrame(columns=['DATE'])\n",
    "\n",
    "# Add rows with dates to the DataFrame\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "dates_to_include = date_range[(date_range.month == 12) | (date_range.month == 1)]\n",
    "compiled_df['DATE'] = dates_to_include.strftime('%m-%d')\n",
    "\n",
    "# Add columns for each year\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    compiled_df[year] = ''\n",
    "\n",
    "# Fill in TAVG values into the DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    for index, row in df.iterrows():\n",
    "        if row['DATE'].strftime('%m-%d') in compiled_df['DATE'].values:\n",
    "            date_index = compiled_df.index[compiled_df['DATE'] == row['DATE'].strftime('%m-%d')][0]\n",
    "            compiled_df.at[date_index, year] = row['TAVG']\n",
    "\n",
    "# Export compiled DataFrame to CSV in the 'data/' folder\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "compiled_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "971366a2-3f32-4149-aa56-5bd4a2cac55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load JSON data from file\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "# Function to filter data within date range and calculate TAVG\n",
    "def process_data(json_data, start_date, end_date):\n",
    "    df = pd.DataFrame(json_data['results'])\n",
    "    df['DATE'] = pd.to_datetime(df['date'])\n",
    "    df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
    "    \n",
    "    tmax_values = df[df['datatype'] == 'TMAX']['value']\n",
    "    tmin_values = df[df['datatype'] == 'TMIN']['value']\n",
    "    \n",
    "    # Convert values to numeric to avoid errors\n",
    "    tmax_values = pd.to_numeric(tmax_values, errors='coerce')\n",
    "    tmin_values = pd.to_numeric(tmin_values, errors='coerce')\n",
    "    \n",
    "    # Calculate TAVG only if both TMAX and TMIN are available\n",
    "    df['TAVG'] = (tmax_values + tmin_values) / 2\n",
    "    return df[['DATE', 'TAVG']]\n",
    "\n",
    "# Define date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    json_data = load_json_file(os.path.join(data_dir, file))\n",
    "    df = process_data(json_data, start_date, end_date)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Create empty DataFrame to store compiled data\n",
    "compiled_df = pd.DataFrame(columns=['DATE'])\n",
    "\n",
    "# Add rows with dates to the DataFrame\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "dates_to_include = date_range[(date_range.month == 12) | (date_range.month == 1)]\n",
    "compiled_df['DATE'] = dates_to_include.strftime('%m-%d')\n",
    "\n",
    "# Add columns for each year\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    compiled_df[year] = ''\n",
    "\n",
    "# Fill in TAVG values into the DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    for index, row in df.iterrows():\n",
    "        if row['DATE'].strftime('%m-%d') in compiled_df['DATE'].values:\n",
    "            date_index = compiled_df.index[compiled_df['DATE'] == row['DATE'].strftime('%m-%d')][0]\n",
    "            compiled_df.at[date_index, year] = row['TAVG']\n",
    "\n",
    "# Export compiled DataFrame to CSV in the 'data/' folder\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "compiled_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d70dcff4-9903-4cc0-aada-35916316a3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load JSON data from file\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "# Function to filter data within date range and calculate TAVG\n",
    "def process_data(json_data, start_date, end_date):\n",
    "    df = pd.DataFrame(json_data['results'])\n",
    "    df['DATE'] = pd.to_datetime(df['date'])\n",
    "    df = df[(df['DATE'] >= start_date) & (df['DATE'] <= end_date)]\n",
    "    \n",
    "    tmax_values = df[df['datatype'] == 'TMAX']['value']\n",
    "    tmin_values = df[df['datatype'] == 'TMIN']['value']\n",
    "    \n",
    "    # Convert values to numeric to avoid errors\n",
    "    tmax_values = pd.to_numeric(tmax_values, errors='coerce')\n",
    "    tmin_values = pd.to_numeric(tmin_values, errors='coerce')\n",
    "    \n",
    "    # Calculate TAVG only if both TMAX and TMIN are available\n",
    "    df['TAVG'] = (tmax_values + tmin_values) / 2\n",
    "    return df[['DATE', 'TAVG']]\n",
    "\n",
    "# Define date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    json_data = load_json_file(os.path.join(data_dir, file))\n",
    "    df = process_data(json_data, start_date, end_date)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Create empty DataFrame to store compiled data\n",
    "compiled_df = pd.DataFrame(columns=['DATE'])\n",
    "\n",
    "# Add rows with dates to the DataFrame\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "dates_to_include = date_range[(date_range.month == 12) | (date_range.month == 1)]\n",
    "compiled_df['DATE'] = dates_to_include.strftime('%m-%d')\n",
    "\n",
    "# Add columns for each year\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    compiled_df[year] = ''\n",
    "\n",
    "# Fill in TAVG values into the DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    year = str(start_date.year + i) + '-' + str(start_date.year + i + 1)\n",
    "    for index, row in df.iterrows():\n",
    "        date_index = compiled_df.index[compiled_df['DATE'] == row['DATE'].strftime('%m-%d')][0]\n",
    "        compiled_df.at[date_index, year] = row['TAVG']\n",
    "\n",
    "# Export compiled DataFrame to CSV in the 'data/' folder\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "compiled_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73903c49-7e2d-4992-95f4-a7984f4f914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TAVG for each date\n",
    "tavg_df = combined_df[combined_df['datatype'] == 'TAVG'][['DATE', 'value']]\n",
    "tavg_df.rename(columns={'value': 'TAVG'}, inplace=True)\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = tavg_df.pivot_table(index=tavg_df['DATE'].dt.strftime('%m-%d'), columns=tavg_df['DATE'].dt.strftime('%Y-%Y'), values='TAVG')\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f652c7a-a191-43f3-b172-79e7a80c8d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TAVG for each date\n",
    "tavg_df = combined_df[combined_df['datatype'] == 'TAVG'][['DATE', 'value']]\n",
    "tavg_df.rename(columns={'value': 'TAVG'}, inplace=True)\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = tavg_df.pivot_table(index=tavg_df['DATE'].dt.strftime('%m-%d'), columns=tavg_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\"), values='TAVG')\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cc9f70e-de38-49e4-ab7d-4799a37651cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TAVG for each date\n",
    "tavg_df = combined_df[combined_df['datatype'] == 'TAVG'][['DATE', 'value']]\n",
    "tavg_df.rename(columns={'value': 'TAVG'}, inplace=True)\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = tavg_df.pivot_table(index=tavg_df['DATE'].dt.strftime('%m-%d'), columns=tavg_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\"), values='TAVG')\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cea74616-5026-4a87-a0d7-67757e836dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TAVG for each date\n",
    "tavg_df = combined_df[combined_df['datatype'] == 'TAVG'][['DATE', 'value']]\n",
    "tavg_df.rename(columns={'value': 'TAVG'}, inplace=True)\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = tavg_df.pivot_table(index=tavg_df['DATE'].dt.strftime('%m-%d'), columns=tavg_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\"), values='TAVG', aggfunc='first')\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28bb5847-b284-45bf-8e9d-f95899636286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TAVG for each date\n",
    "tavg_df = combined_df[combined_df['datatype'] == 'TAVG'][['DATE', 'value']]\n",
    "tavg_df.rename(columns={'value': 'TAVG'}, inplace=True)\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = tavg_df.pivot_table(index=tavg_df['DATE'].dt.strftime('%m-%d'), columns=tavg_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\"), values='TAVG', aggfunc='first')\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68bf0677-98c3-4921-97aa-2d678c12c5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TAVG for each date\n",
    "tavg_df = combined_df[combined_df['datatype'] == 'TAVG'][['DATE', 'value']]\n",
    "tavg_df.rename(columns={'value': 'TAVG'}, inplace=True)\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = tavg_df.pivot_table(index=tavg_df['DATE'].dt.strftime('%m-%d'), columns=tavg_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\"), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "pivoted_df = pivoted_df.reindex(columns=[f\"{year}-{year+1}\" for year in range(start_year, end_year+1)])\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d0990cf-63c4-4118-a5bb-fc7780047b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\"), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates in the correct order\n",
    "index_dates = [f\"{month:02d}-{day:02d}\" for month in range(12, 13) for day in range(15, 32)]  # December\n",
    "index_dates += [f\"{month:02d}-{day:02d}\" for month in range(1, 2) for day in range(1, 22)]     # January\n",
    "\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f4b0f9d-36b4-4978-b8fe-479e7bf1842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\" if x <= 2021 else ''), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates in the correct order\n",
    "index_dates = [f\"{month:02d}-{day:02d}\" for month in range(12, 13) for day in range(15, 32)]  # December\n",
    "index_dates += [f\"{month:02d}-{day:02d}\" for month in range(1, 2) for day in range(1, 22)]     # January\n",
    "\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "403f5ede-bbc0-45bd-a056-8a23da7b99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & (combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\" if x <= 2021 else ''), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates in the correct order\n",
    "index_dates = [f\"{month:02d}-{day:02d}\" for month in range(12, 13) for day in range(15, 32)]  # December\n",
    "index_dates += [f\"{month:02d}-{day:02d}\" for month in range(1, 2) for day in range(1, 22)]     # January\n",
    "\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36685a9b-1370-476e-bc8b-637be8f74f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\" if x <= 2021 else ''), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates in the correct order\n",
    "index_dates = [f\"{month:02d}-{day:02d}\" for month in range(12, 13) for day in range(15, 32)]  # December\n",
    "index_dates += [f\"{month:02d}-{day:02d}\" for month in range(1, 2) for day in range(1, 22)]     # January\n",
    "\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed15ef90-f323-44b2-b4e8-8fbb3c7d9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\" if x <= 2021 else ''), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates in the correct order\n",
    "index_dates = [f\"{month:02d}-{day:02d}\" for month in range(1, 2) for day in range(1, 22)]     # January\n",
    "index_dates += [f\"{month:02d}-{day:02d}\" for month in range(12, 13) for day in range(15, 32)]  # December\n",
    "\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa329adc-3a23-4837-aa29-41f8d920a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n",
      "Average Temperature: 30.290412305272984\n",
      "Seasonal Average of Maximum Temperature: nan\n",
      "Seasonal Average of Minimum Temperature: nan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def average(df):\n",
    "    \"\"\"\n",
    "    Compute the average of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Average temperature.\n",
    "    \"\"\"\n",
    "    return df.mean().mean()\n",
    "\n",
    "\n",
    "def average_warmest(df):\n",
    "    \"\"\"\n",
    "    Compute the seasonal average of the maximum temperature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Seasonal average of the maximum temperature.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the seasonal range\n",
    "    seasonal_df = df.loc['12-15':'01-21']\n",
    "\n",
    "    # Get the maximum temperature within the seasonal range\n",
    "    max_temp = seasonal_df.max().max()\n",
    "\n",
    "    return max_temp\n",
    "\n",
    "\n",
    "def average_coldest(df):\n",
    "    \"\"\"\n",
    "    Compute the seasonal average of the minimum temperature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Seasonal average of the minimum temperature.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the seasonal range\n",
    "    seasonal_df = df.loc['12-15':'01-21']\n",
    "\n",
    "    # Get the minimum temperature within the seasonal range\n",
    "    min_temp = seasonal_df.min().min()\n",
    "\n",
    "    return min_temp\n",
    "\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\" if x <= 2021 else ''), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates in the correct order\n",
    "index_dates = [f\"{month:02d}-{day:02d}\" for month in range(1, 2) for day in range(1, 22)]     # January\n",
    "index_dates += [f\"{month:02d}-{day:02d}\" for month in range(12, 13) for day in range(15, 32)]  # December\n",
    "\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n",
    "\n",
    "# Compute the average temperature\n",
    "avg_temp = average(pivoted_df)\n",
    "print(\"Average Temperature:\", avg_temp)\n",
    "\n",
    "# Compute the seasonal average of the maximum temperature\n",
    "avg_max_temp = average_warmest(merged_df)\n",
    "print(\"Seasonal Average of Maximum Temperature:\", avg_max_temp)\n",
    "\n",
    "# Compute the seasonal average of the minimum temperature\n",
    "avg_min_temp = average_coldest(merged_df)\n",
    "print(\"Seasonal Average of Minimum Temperature:\", avg_min_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ad24d0ce-e3ba-4c45-8ec8-3d00d6a8cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\" if x <= 2021 else ''), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates in the correct order\n",
    "index_dates = [f\"{month:02d}-{day:02d}\" for month in range(12, 13) for day in range(15, 32)]  # December\n",
    "index_dates += [f\"{month:02d}-{day:02d}\" for month in range(1, 2) for day in range(1, 22)]     # January\n",
    "\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Sort by month and day\n",
    "pivoted_df.sort_index(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aad16cf1-043c-4677-935c-f7e2487b7fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Temperature: 30.629126213592233\n",
      "Seasonal Average of Maximum Temperature: nan\n",
      "Seasonal Average of Minimum Temperature: nan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def average(df):\n",
    "    \"\"\"\n",
    "    Compute the average of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Average temperature.\n",
    "    \"\"\"\n",
    "    return df.mean().mean()\n",
    "\n",
    "\n",
    "def average_warmest(df):\n",
    "    \"\"\"\n",
    "    Compute the seasonal average of the maximum temperature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Seasonal average of the maximum temperature.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the seasonal range\n",
    "    seasonal_df = df.loc['01-01':'01-21']\n",
    "\n",
    "    # Get the maximum temperature within the seasonal range\n",
    "    max_temp = seasonal_df['TMAX'].mean()\n",
    "\n",
    "    return max_temp\n",
    "\n",
    "\n",
    "def average_coldest(df):\n",
    "    \"\"\"\n",
    "    Compute the seasonal average of the minimum temperature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Seasonal average of the minimum temperature.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the seasonal range\n",
    "    seasonal_df = df.loc['12-15':'12-31']\n",
    "\n",
    "    # Get the minimum temperature within the seasonal range\n",
    "    min_temp = seasonal_df['TMIN'].mean()\n",
    "\n",
    "    return min_temp\n",
    "\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Compute the average temperature\n",
    "avg_temp = average(merged_df)\n",
    "print(\"Average Temperature:\", avg_temp)\n",
    "\n",
    "# Compute the seasonal average of the maximum temperature\n",
    "avg_max_temp = average_warmest(merged_df)\n",
    "print(\"Seasonal Average of Maximum Temperature:\", avg_max_temp)\n",
    "\n",
    "# Compute the seasonal average of the minimum temperature\n",
    "avg_min_temp = average_coldest(merged_df)\n",
    "print(\"Seasonal Average of Minimum Temperature:\", avg_min_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fb158-e36e-4858-92f3-09ccd3dbe7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sau24s]",
   "language": "python",
   "name": "conda-env-sau24s-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
