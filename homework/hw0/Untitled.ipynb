{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e93d6cc-aac2-4487-8854-04a39ec48ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/envs/sau24s/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d2ec5d-e0dd-45bf-9e0d-6988739c9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/envs/sau24s/lib/python3.8/site-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54aee0b0-68e3-4cbd-8f04-ef3a44613487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24afe287-2288-4071-9145-2773881d9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ce40a6-a9ef-45c2-b9a4-7259cbac0b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2008-12-15-2009-01-21 saved successfully.\n",
      "Data for 2009-12-15-2010-01-21 saved successfully.\n",
      "Data for 2010-12-15-2011-01-21 saved successfully.\n",
      "Data for 2011-12-15-2012-01-21 saved successfully.\n",
      "Data for 2012-12-15-2013-01-21 saved successfully.\n",
      "Error fetching data: 502 Server Error: Proxy Error for url: https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:80249&units=standard&startdate=2013-12-15&enddate=2014-01-21&limit=1000\n",
      "Data for 2014-12-15-2015-01-21 saved successfully.\n",
      "Data for 2015-12-15-2016-01-21 saved successfully.\n",
      "Data for 2016-12-15-2017-01-21 saved successfully.\n",
      "Data for 2017-12-15-2018-01-21 saved successfully.\n",
      "Data for 2018-12-15-2019-01-21 saved successfully.\n",
      "Data for 2019-12-15-2020-01-21 saved successfully.\n",
      "Data for 2020-12-15-2021-01-21 saved successfully.\n",
      "Data for 2021-12-15-2022-01-21 saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "def fetch_data(start_date, end_date, file_name):\n",
    "    token = 'peUoGReGomLyRqAedzPNFbpUtrtznRun'\n",
    "    url = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:80249&units=standard&startdate={start_date}&enddate={end_date}&limit=1000\"\n",
    "    headers = {\"token\": token}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        with open(os.path.join(data_dir, file_name), 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        print(f\"Data for {start_date}-{end_date} saved successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "\n",
    "# Define the date ranges\n",
    "date_ranges = [\n",
    "    (\"2008-12-15\", \"2009-01-21\"),\n",
    "    (\"2009-12-15\", \"2010-01-21\"),\n",
    "    (\"2010-12-15\", \"2011-01-21\"),\n",
    "    (\"2011-12-15\", \"2012-01-21\"),\n",
    "    (\"2012-12-15\", \"2013-01-21\"),\n",
    "    (\"2013-12-15\", \"2014-01-21\"),\n",
    "    (\"2014-12-15\", \"2015-01-21\"),\n",
    "    (\"2015-12-15\", \"2016-01-21\"),\n",
    "    (\"2016-12-15\", \"2017-01-21\"),\n",
    "    (\"2017-12-15\", \"2018-01-21\"),\n",
    "    (\"2018-12-15\", \"2019-01-21\"),\n",
    "    (\"2019-12-15\", \"2020-01-21\"),\n",
    "    (\"2020-12-15\", \"2021-01-21\"),\n",
    "    (\"2021-12-15\", \"2022-01-21\")\n",
    "]\n",
    "\n",
    "# Fetch data for each date range and save it to a JSON file\n",
    "for i, (start_date, end_date) in enumerate(date_ranges, 1):\n",
    "    file_name = f'winter_{start_date[:4]}-{end_date[:4]}.json'\n",
    "    fetch_data(start_date, end_date, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974f58bb-5feb-4288-b870-40f9b2c3048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/envs/sau24s/lib/python3.8/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from requests) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df93ced-60d8-4111-a404-dae5d6f4307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2008-12-15-2009-01-21 saved successfully.\n",
      "Data for 2009-12-15-2010-01-21 saved successfully.\n",
      "Data for 2010-12-15-2011-01-21 saved successfully.\n",
      "Data for 2011-12-15-2012-01-21 saved successfully.\n",
      "Data for 2012-12-15-2013-01-21 saved successfully.\n",
      "Data for 2013-12-15-2014-01-21 saved successfully.\n",
      "Data for 2014-12-15-2015-01-21 saved successfully.\n",
      "Data for 2015-12-15-2016-01-21 saved successfully.\n",
      "Data for 2016-12-15-2017-01-21 saved successfully.\n",
      "Data for 2017-12-15-2018-01-21 saved successfully.\n",
      "Data for 2018-12-15-2019-01-21 saved successfully.\n",
      "Data for 2019-12-15-2020-01-21 saved successfully.\n",
      "Data for 2020-12-15-2021-01-21 saved successfully.\n",
      "Data for 2021-12-15-2022-01-21 saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "def fetch_data(start_date, end_date, file_name):\n",
    "    token = 'peUoGReGomLyRqAedzPNFbpUtrtznRun'\n",
    "    url = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:80249&units=standard&startdate={start_date}&enddate={end_date}&limit=1000\"\n",
    "    headers = {\"token\": token}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        with open(os.path.join(data_dir, file_name), 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        print(f\"Data for {start_date}-{end_date} saved successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "\n",
    "# Define the date ranges\n",
    "date_ranges = [\n",
    "    (\"2008-12-15\", \"2009-01-21\"),\n",
    "    (\"2009-12-15\", \"2010-01-21\"),\n",
    "    (\"2010-12-15\", \"2011-01-21\"),\n",
    "    (\"2011-12-15\", \"2012-01-21\"),\n",
    "    (\"2012-12-15\", \"2013-01-21\"),\n",
    "    (\"2013-12-15\", \"2014-01-21\"),\n",
    "    (\"2014-12-15\", \"2015-01-21\"),\n",
    "    (\"2015-12-15\", \"2016-01-21\"),\n",
    "    (\"2016-12-15\", \"2017-01-21\"),\n",
    "    (\"2017-12-15\", \"2018-01-21\"),\n",
    "    (\"2018-12-15\", \"2019-01-21\"),\n",
    "    (\"2019-12-15\", \"2020-01-21\"),\n",
    "    (\"2020-12-15\", \"2021-01-21\"),\n",
    "    (\"2021-12-15\", \"2022-01-21\")\n",
    "]\n",
    "\n",
    "# Fetch data for each date range and save it to a JSON file\n",
    "for i, (start_date, end_date) in enumerate(date_ranges, 1):\n",
    "    file_name = f'winter_{start_date[:4]}-{end_date[:4]}.json'\n",
    "    try:\n",
    "        fetch_data(start_date, end_date, file_name)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "        print(f\"Data for {start_date}-{end_date} could not be fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a24c465-44f2-49dd-a71f-2fc8c768db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Extract required information and calculate TAVG\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df['TMAX'] = combined_df['value'][combined_df['datatype'] == 'TMAX'].reset_index(drop=True)\n",
    "combined_df['TMIN'] = combined_df['value'][combined_df['datatype'] == 'TMIN'].reset_index(drop=True)\n",
    "combined_df = combined_df.groupby('DATE').agg({'TMAX': 'max', 'TMIN': 'min'}).reset_index()\n",
    "combined_df['TAVG'] = (combined_df['TMAX'] + combined_df['TMIN']) / 2\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c257515-a532-447c-ba2c-2d697fd5e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2b7194-f1a7-4ec7-b223-0b437b1abab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Sort DataFrame by DATE\n",
    "merged_df.sort_values(by='DATE', inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a87c54-05fe-4227-877f-4cfc480ad46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load and preprocess data from a JSON file\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        df['DATE'] = pd.to_datetime(df['date'])\n",
    "        df['TAVG'] = df['value']  # Assuming 'TAVG' is already in the required unit/format\n",
    "    return df[['DATE', 'TAVG']]\n",
    "\n",
    "# Combine all TAVG data into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "tavg_dfs = []\n",
    "\n",
    "for file in json_files:\n",
    "    tavg_df = load_json_file(os.path.join(data_dir, file))\n",
    "    tavg_dfs.append(tavg_df)\n",
    "\n",
    "combined_tavg_df = pd.concat(tavg_dfs, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_tavg_df = combined_tavg_df[(combined_tavg_df['DATE'] >= start_date) & (combined_tavg_df['DATE'] <= end_date)]\n",
    "\n",
    "# Create a DataFrame with rows as dates and columns as years, and fill it with TAVG values\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "years = pd.Series(date_range.year).unique()\n",
    "year_columns = [f'{year}-{year+1}' for year in years]\n",
    "date_index = [date.strftime('%m-%d') for date in date_range]\n",
    "\n",
    "compiled_data = pd.DataFrame(index=date_index, columns=year_columns)\n",
    "\n",
    "for year in years:\n",
    "    year_start = pd.to_datetime(f'{year}-12-15')\n",
    "    year_end = pd.to_datetime(f'{year+1}-01-21')\n",
    "    year_data = combined_tavg_df[(combined_tavg_df['DATE'] >= year_start) & (combined_tavg_df['DATE'] <= year_end)]\n",
    "    for date in date_index:\n",
    "        avg_tavg = year_data[year_data['DATE'].dt.strftime('%m-%d') == date]['TAVG'].mean()\n",
    "        compiled_data.at[date, f'{year}-{year+1}'] = round(avg_tavg, 2)\n",
    "\n",
    "# Export compiled data to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "compiled_data.to_csv(output_file)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d1089e0-f160-42bf-b0d6-6a5640ff0b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df[(combined_df['DATE'] >= start_date) & (combined_df['DATE'] <= end_date)]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "629e5148-7c08-4140-a7ee-a154819288b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_MAX_MIN_AVG.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Sort DataFrame by DATE\n",
    "merged_df.sort_values(by='DATE', inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = 'data/all_data_MAX_MIN_AVG.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82670d05-bada-4657-ab73-225d1de5357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to data/all_data_avg.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Pivot table to have years as columns and dates as rows\n",
    "pivoted_df = merged_df.pivot_table(index=merged_df['DATE'].dt.strftime('%m-%d'), columns=merged_df['DATE'].dt.year.apply(lambda x: f\"{x}-{x+1}\" if x <= 2021 else ''), values='TAVG')\n",
    "\n",
    "# Reindex to include all years between '2008-2009' and '2021-2022'\n",
    "start_year = 2008\n",
    "end_year = 2021\n",
    "\n",
    "# Define the index_dates for December\n",
    "index_dates_december = [f\"{month:02d}-{day:02d}\" for day in range(15, 32) for month in range(12, 13)]  # December\n",
    "# Define the index_dates for January\n",
    "index_dates_january = [f\"{month:02d}-{day:02d}\" for day in range(1, 22) for month in range(1, 2)]     # January\n",
    "\n",
    "# Concatenate December and January index dates\n",
    "index_dates = index_dates_december + index_dates_january\n",
    "\n",
    "# Reindex the pivoted DataFrame\n",
    "pivoted_df = pivoted_df.reindex(index=index_dates)\n",
    "\n",
    "# Check if \"2008-2009\" column already exists\n",
    "if \"2008-2009\" not in pivoted_df.columns:\n",
    "    # Add the \"2008-2009\" column with empty values\n",
    "    pivoted_df.insert(0, \"2008-2009\", \"\")\n",
    "\n",
    "# Export to CSV\n",
    "output_file = os.path.join(data_dir, 'all_data_avg.csv')\n",
    "pivoted_df.to_csv(output_file, index_label='DATE')\n",
    "\n",
    "print(f\"CSV file exported successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9ca9d25-8d12-4798-8ffe-70b92deabe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Temperature: 30.629126213592233\n",
      "Seasonal Average of Maximum Temperature: 43.83883495145631\n",
      "Seasonal Average of Minimum Temperature: 17.419417475728157\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def average(df):\n",
    "    \"\"\"\n",
    "    Compute the average temperature of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Average temperature.\n",
    "    \"\"\"\n",
    "    return df.mean().mean()\n",
    "\n",
    "def average_warmest(df):\n",
    "    \"\"\"\n",
    "    Compute the seasonal average of the maximum temperature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Seasonal average of the maximum temperature.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the seasonal range starting from '12-15' to '1-21'\n",
    "    seasonal_df = df[(df['DATE'].dt.month == 12) & (df['DATE'].dt.day >= 15) | (df['DATE'].dt.month == 1) & (df['DATE'].dt.day <= 21)]\n",
    "    \n",
    "    # Get the maximum temperature within the seasonal range\n",
    "    max_temp = seasonal_df['TMAX'].mean()\n",
    "\n",
    "    return max_temp\n",
    "\n",
    "def average_coldest(df):\n",
    "    \"\"\"\n",
    "    Compute the seasonal average of the minimum temperature in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing temperature data.\n",
    "\n",
    "    Returns:\n",
    "    float: Seasonal average of the minimum temperature.\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the seasonal range starting from '12-15' to '1-21'\n",
    "    seasonal_df = df[(df['DATE'].dt.month == 12) & (df['DATE'].dt.day >= 15) | (df['DATE'].dt.month == 1) & (df['DATE'].dt.day <= 21)]\n",
    "    \n",
    "    # Get the minimum temperature within the seasonal range\n",
    "    min_temp = seasonal_df['TMIN'].mean()\n",
    "\n",
    "    return min_temp\n",
    "\n",
    "# Combine all JSON files into a single DataFrame\n",
    "data_dir = 'data'\n",
    "json_files = [file for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "dataframes = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(data_dir, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        df = pd.DataFrame(json_data['results'])\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Filter data within the required date range excluding December 2013 and January 2014\n",
    "start_date = pd.to_datetime('2008-12-15')\n",
    "end_date = pd.to_datetime('2022-01-21')\n",
    "combined_df['DATE'] = pd.to_datetime(combined_df['date'])\n",
    "\n",
    "# Excluding December 2013 and January 2014\n",
    "combined_df = combined_df[~((combined_df['DATE'].dt.year == 2013) & ((combined_df['DATE'].dt.month == 12) | (combined_df['DATE'].dt.year == 2014) & (combined_df['DATE'].dt.month == 1)))]\n",
    "\n",
    "# Extract TMAX and TMIN for each date\n",
    "tmax_df = combined_df[combined_df['datatype'] == 'TMAX'].rename(columns={'value': 'TMAX'})[['DATE', 'TMAX']]\n",
    "tmin_df = combined_df[combined_df['datatype'] == 'TMIN'].rename(columns={'value': 'TMIN'})[['DATE', 'TMIN']]\n",
    "\n",
    "# Merge TMAX and TMIN data\n",
    "merged_df = pd.merge(tmax_df, tmin_df, on='DATE')\n",
    "\n",
    "# Calculate TAVG\n",
    "merged_df['TAVG'] = (merged_df['TMAX'] + merged_df['TMIN']) / 2\n",
    "\n",
    "# Compute the average temperature\n",
    "avg_temp = average(merged_df)\n",
    "print(\"Average Temperature:\", avg_temp)\n",
    "\n",
    "# Compute the seasonal average of the maximum temperature\n",
    "avg_max_temp = average_warmest(merged_df)\n",
    "print(\"Seasonal Average of Maximum Temperature:\", avg_max_temp)\n",
    "\n",
    "# Compute the seasonal average of the minimum temperature\n",
    "avg_min_temp = average_coldest(merged_df)\n",
    "print(\"Seasonal Average of Minimum Temperature:\", avg_min_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7b108-b73a-4c0b-8812-27c8066dd9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
    "display_name": "Python [conda env:sau24s]",
    "language": "python",
    "name": "conda-env-sau24s-py"
},
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
