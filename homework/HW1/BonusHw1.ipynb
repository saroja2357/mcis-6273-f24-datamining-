{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd729c8-4d81-48d5-8cf1-c6b96356083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/envs/sau24s/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sau24s/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d703b2f5-d79d-427b-a90b-5afc1885b478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to Baltimore_Washington_data.json\n"
     ]
    }
   ],
   "source": [
    "#Baltimore/Washington International: 39.1754, -76.668297\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"https://api.openaq.org/v2/measurements\"\n",
    "\n",
    "# Define the parameters for the request\n",
    "parameters = {\n",
    "    \"parameter\": \"pm25\",  # PM2.5 data only\n",
    "    \"date_from\": \"2023-05-01\",\n",
    "    \"date_to\": \"2023-08-31\",\n",
    "    \"coordinates\": \"39.1754,-76.668297\",\n",
    "    \"radius\": 7500,  # 7.5km radius\n",
    "    \"limit\": 80000  # Limit the number of data points\n",
    "}\n",
    "\n",
    "# Send the GET request to the API\n",
    "response = requests.get(url, params=parameters)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Save the data to a JSON file\n",
    "    with open('Baltimore_Washington_data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    print(\"Data saved successfully to Baltimore_Washington_data.json\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e44956b4-7993-4815-b37e-e7a41ccc00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in 'Baltimore_Washington_7.5km_data.csv' successfully.\n"
     ]
    }
   ],
   "source": [
    "####    Task: Transform, filter and store the OpenAQ data as CSV\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('Baltimore_Washington_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize the 'results' part of the JSON data\n",
    "results = data['results']\n",
    "df = pd.json_normalize(results)\n",
    "\n",
    "# Convert the date field to datetime64 if it exists\n",
    "if 'date.local' in df.columns:\n",
    "    df['local_time'] = pd.to_datetime(df['date.local'])\n",
    "\n",
    "# Check if the necessary columns are present in the DataFrame\n",
    "expected_columns = ['location', 'parameter', 'value', 'local_time']\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "# If any columns are missing, print a message and handle the issue accordingly\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    # Extract latitude and longitude from the nested 'coordinates' fields\n",
    "    df['sensor_lat'] = df['coordinates.latitude']\n",
    "    df['sensor_lon'] = df['coordinates.longitude']\n",
    "\n",
    "    # Reduce the DataFrame to include only specified fields\n",
    "    df = df[expected_columns + ['sensor_lat', 'sensor_lon']]\n",
    "\n",
    "    # Store it to a file called: 20230606_detroit_downtown_7_5km_aq.csv\n",
    "    filename = 'Baltimore_Washington_7.5km_data.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data stored in '{filename}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e4503d7-1d2b-4313-a492-670f703f11bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Density for Baltimore/Washington International: 655\n",
      "Intensity of PM2.5 for Baltimore/Washington International: 37.30946564885496\n",
      "Sensor Density for Detroit: 5186\n",
      "Intensity of PM2.5 for Detroit: 72.8891245661396\n"
     ]
    }
   ],
   "source": [
    "# Sensordensity and intensity of PM 2.5 on June 6 2023\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from both datasets into DataFrames\n",
    "baltimore_df = pd.read_csv('Baltimore_Washington_7.5km_data.csv')\n",
    "detroit_df = pd.read_csv('20230606_detroit_downtown_7_5km_aq.csv')\n",
    "\n",
    "# Filter data for June 6, 2023\n",
    "baltimore_june6_df = baltimore_df[baltimore_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "detroit_june6_df = detroit_df[detroit_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "\n",
    "# Calculate sensor density for Baltimore/Washington International\n",
    "baltimore_sensor_density = len(baltimore_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Baltimore/Washington International\n",
    "baltimore_pm25_intensity = baltimore_june6_df['value'].mean()\n",
    "\n",
    "# Calculate sensor density for Detroit\n",
    "detroit_sensor_density = len(detroit_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Detroit\n",
    "detroit_pm25_intensity = detroit_june6_df['value'].mean()\n",
    "\n",
    "print(\"Sensor Density for Baltimore/Washington International:\", baltimore_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Baltimore/Washington International:\", baltimore_pm25_intensity)\n",
    "print(\"Sensor Density for Detroit:\", detroit_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Detroit:\", detroit_pm25_intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2fb05-3f0e-4912-bd15-89c1648d5678",
   "metadata": {},
   "source": [
    "Similarites:\n",
    "\n",
    "Both locations possess functioning air quality monitoring systems, evident from their reported sensor density values. Additionally, they have recorded PM2.5 levels, indicating active efforts to monitor and assess air pollution levels. \n",
    "These similarities underscore shared initiatives to monitor and mitigate air pollution in urban environments.\n",
    "\n",
    "Differences:\n",
    "\n",
    "Detroit demonstrates a notably higher sensor density of 5186 compared to Baltimore/Washington International's 655, suggesting a more extensive air quality monitoring network in Detroit.\n",
    "Additionally, Detroit reports a higher intensity of PM2.5 at 72.89 µg/m³, contrasting with Baltimore/Washington International's lower PM2.5 intensity of 37.31 µg/m³, indicating varying levels of air pollution severity between the two locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94696f-e862-4b59-82f0-7cb996444142",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.openaq.org/v2/measurements?date_from=2023-05-01%2000%3A00%3A00&date_to=2023-08-31%2023%3A59%3A59&limit=50000&page=1&offset=0&sort=desc&parameter_id=2&coordinates=42.3643%2C-71.005203&radius=7500&order_by=datetime\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27835d8-b11c-442e-8655-f5369d994bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to Boston_data.json\n"
     ]
    }
   ],
   "source": [
    "## Boston Logan International: 42.3643, -71.005203\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"https://api.openaq.org/v2/measurements\"\n",
    "\n",
    "# Define the parameters for the request\n",
    "parameters = {\n",
    "    \"parameter\": \"pm25\",  # PM2.5 data only\n",
    "    \"date_from\": \"2023-05-01T00:00:00\",  # Include time for start date\n",
    "    \"date_to\": \"2023-08-31T23:59:59\",    # Include time for end date\n",
    "    \"coordinates\": \"42.3643,-71.005203\",\n",
    "    \"radius\": 7500,  # 7.5km radius\n",
    "    \"limit\": 35000\n",
    "}\n",
    "\n",
    "# Send the GET request to the API\n",
    "response = requests.get(url, params=parameters)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "\n",
    "    # Save the data to a JSON file\n",
    "    with open('Boston_data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    print(\"Data saved successfully to Boston_data.json\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b6ddf11-c53c-47cc-a7b6-38ef92c03467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in 'Boston_Logan_7_5km_aq.csv' successfully.\n"
     ]
    }
   ],
   "source": [
    "####    Task: Transform, filter and store the OpenAQ data as CSV\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('Boston_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize the 'results' part of the JSON data\n",
    "results = data['results']\n",
    "df = pd.json_normalize(results)\n",
    "\n",
    "# Convert the date field to datetime64 if it exists\n",
    "if 'date.local' in df.columns:\n",
    "    df['local_time'] = pd.to_datetime(df['date.local'])\n",
    "\n",
    "# Check if the necessary columns are present in the DataFrame\n",
    "expected_columns = ['location', 'parameter', 'value', 'local_time']\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "# If any columns are missing, print a message and handle the issue accordingly\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    # Extract latitude and longitude from the nested 'coordinates' fields\n",
    "    df['sensor_lat'] = df['coordinates.latitude']\n",
    "    df['sensor_lon'] = df['coordinates.longitude']\n",
    "\n",
    "    # Reduce the DataFrame to include only specified fields\n",
    "    df = df[expected_columns + ['sensor_lat', 'sensor_lon']]\n",
    "\n",
    "    # Store it to a file called: 20230606_detroit_downtown_7_5km_aq.csv\n",
    "    filename = 'Boston_Logan_7_5km_aq.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data stored in '{filename}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa772278-9caa-4e89-a88e-95fdfff6583f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Density for Boston International: 692\n",
      "Intensity of PM2.5 for Boston International: 29.277167630057804\n",
      "Sensor Density for Detroit: 5186\n",
      "Intensity of PM2.5 for Detroit: 72.8891245661396\n"
     ]
    }
   ],
   "source": [
    "# Sensor density and intensity of PM 2.5 on June 6 2023\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from both datasets into DataFrames\n",
    "boston_df = pd.read_csv('Boston_Logan_7_5km_aq.csv')\n",
    "detroit_df = pd.read_csv('20230606_detroit_downtown_7_5km_aq.csv')\n",
    "\n",
    "# Filter data for June 6, 2023\n",
    "boston_june6_df = boston_df[boston_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "detroit_june6_df = detroit_df[detroit_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "\n",
    "# Calculate sensor density for Boston International\n",
    "boston_sensor_density = len(boston_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Boston International\n",
    "boston_pm25_intensity = boston_june6_df['value'].mean()\n",
    "\n",
    "# Calculate sensor density for Detroit\n",
    "detroit_sensor_density = len(detroit_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Detroit\n",
    "detroit_pm25_intensity = detroit_june6_df['value'].mean()\n",
    "\n",
    "print(\"Sensor Density for Boston International:\", boston_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Boston International:\", boston_pm25_intensity)\n",
    "print(\"Sensor Density for Detroit:\", detroit_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Detroit:\", detroit_pm25_intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cafc41-87b4-40c3-b527-09d9d5b673a6",
   "metadata": {},
   "source": [
    "\n",
    "Similarites:\n",
    "Both Boston International and Detroit monitor air quality on June 06 related to PM2.5 pollution.\n",
    "Both cities utilize monitoring systems to track air quality data.\n",
    "\n",
    "Differences:\n",
    "Boston International has a lower sensor density (692) compared to Detroit (5186).\n",
    "\n",
    "Detroit experiences a significantly higher intensity of PM2.5 pollution (72.889 µg/m³) compared to Boston International (29.277 µg/m³)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d26b7e7c-a784-451c-ade3-4a64830830a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to PHILA_mayjune_data\n"
     ]
    }
   ],
   "source": [
    "# Philadelphia International Airport:39.8719, -75.2411  \n",
    "# Data from May to June\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time  # Import the time module\n",
    "\n",
    "url = \"https://api.openaq.org/v2/measurements?date_from=2023-05-01%2000%3A00%3A00&date_to=2023-06-30%2023%3A59%3A59&limit=3000&page=1&offset=0&sort=desc&parameter_id=2&coordinates=39.8719%2C-75.2411&radius=7500&order_by=datetime\"\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the response content (JSON) to a file\n",
    "    with open('PHILA_mayjune_data.json', 'w') as json_file:\n",
    "        json.dump(response.json(), json_file, indent=4)\n",
    "    print(\"Data saved to PHILA_mayjune_data\")\n",
    "else:\n",
    "    print(\"Error occurred while fetching data:\", response.text)\n",
    "\n",
    "time.sleep(2)  # Add a sleep time of 2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9c2f5-8538-4f5b-9c01-080621f583e1",
   "metadata": {},
   "source": [
    "Extracted data with date ranges from May to June and July to August and mereged those two into single json file named Philadelphia_data.json. I am unable to extract all the data at once it is throwing error so i broke the daterange itno two different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83814b6b-cb63-4c41-a01f-4d304bbf1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to PHILA_julyaug_data\n"
     ]
    }
   ],
   "source": [
    "# Philadelphia International Airport:39.8719, -75.2411  \n",
    "# Data from July to August\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time  # Import the time module\n",
    "\n",
    "url = \"https://api.openaq.org/v2/measurements?date_from=2023-07-01%2000%3A00%3A00&date_to=2023-08-31%2023%3A59%3A59&limit=3000&page=1&offset=0&sort=desc&parameter_id=2&coordinates=39.8719%2C-75.2411&radius=7500&order_by=datetime\"\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the response content (JSON) to a file\n",
    "    with open('PHILA_julyaug_data.json', 'w') as json_file:\n",
    "        json.dump(response.json(), json_file, indent=4)\n",
    "    print(\"Data saved to PHILA_julyaug_data\")\n",
    "else:\n",
    "    print(\"Error occurred while fetching data:\", response.text)\n",
    "\n",
    "time.sleep(2)  # Add a sleep time of 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2016eef-2fe9-459a-8421-8e8c04f34fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May-June data length: 1382\n",
      "July-August data length: 1443\n",
      "Combined JSON file 'Philadelphia_data.json' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the content of the first JSON file (May-June data)\n",
    "with open('PHILA_mayjune_data.json', 'r') as file:\n",
    "    mayjune_data = json.load(file)\n",
    "    print(\"May-June data length:\", len(mayjune_data[\"results\"]))\n",
    "\n",
    "# Read the content of the second JSON file (July-August data)\n",
    "with open('PHILA_julyaug_data.json', 'r') as file:\n",
    "    julyaug_data = json.load(file)\n",
    "    print(\"July-August data length:\", len(julyaug_data[\"results\"]))\n",
    "\n",
    "# Combine \"results\" data from both JSON files into a single list\n",
    "combined_results = mayjune_data[\"results\"] + julyaug_data[\"results\"]\n",
    "\n",
    "# Construct a new dictionary with combined \"results\"\n",
    "combined_data = {\n",
    "    \"meta\": {\n",
    "        \"source\": \"PHILA_mayjune_data and PHILA_julyaug_data\"\n",
    "    },\n",
    "    \"results\": combined_results\n",
    "}\n",
    "\n",
    "# Write combined data to a JSON file\n",
    "with open('Philadelphia_data.json', 'w') as file:\n",
    "    json.dump(combined_data, file)\n",
    "\n",
    "print(\"Combined JSON file 'Philadelphia_data.json' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95669b11-9fd8-4b9a-908f-7a4407526252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in 'Philadelphia_7.5km_data.csv' successfully.\n"
     ]
    }
   ],
   "source": [
    "####    Task: Transform, filter and store the OpenAQ data as CSV\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('Philadelphia_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize the 'results' part of the JSON data\n",
    "results = data['results']\n",
    "df = pd.json_normalize(results)\n",
    "\n",
    "# Convert the date field to datetime64 if it exists\n",
    "if 'date.local' in df.columns:\n",
    "    df['local_time'] = pd.to_datetime(df['date.local'])\n",
    "\n",
    "# Check if the necessary columns are present in the DataFrame\n",
    "expected_columns = ['location', 'parameter', 'value', 'local_time']\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "# If any columns are missing, print a message and handle the issue accordingly\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    # Extract latitude and longitude from the nested 'coordinates' fields\n",
    "    df['sensor_lat'] = df['coordinates.latitude']\n",
    "    df['sensor_lon'] = df['coordinates.longitude']\n",
    "\n",
    "    # Reduce the DataFrame to include only specified fields\n",
    "    df = df[expected_columns + ['sensor_lat', 'sensor_lon']]\n",
    "\n",
    "    # Store it to a file called: 20230606_detroit_downtown_7_5km_aq.csv\n",
    "    filename = 'Philadelphia_7.5km_data.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data stored in '{filename}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09f6485e-fcc1-4d9f-b872-1b23854713cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Density for Philadelphia International Airport: 10\n",
      "Intensity of PM2.5 for Philadelphia International Airport: 41.19\n",
      "Sensor Density for Detroit: 5186\n",
      "Intensity of PM2.5 for Detroit: 72.8891245661396\n"
     ]
    }
   ],
   "source": [
    "# Sensor density and intensity of PM 2.5 on June 6 2023\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from both datasets into DataFrames\n",
    "Philadelphia_df = pd.read_csv('Philadelphia_7.5km_data.csv')\n",
    "detroit_df = pd.read_csv('20230606_detroit_downtown_7_5km_aq.csv')\n",
    "\n",
    "# Filter data for June 6, 2023\n",
    "Philadelphia_june6_df = Philadelphia_df[Philadelphia_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "detroit_june6_df = detroit_df[detroit_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "\n",
    "# Calculate sensor density for Baltimore/Washington International\n",
    "Philadelphia_sensor_density = len(Philadelphia_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Baltimore/Washington International\n",
    "Philadelphia_pm25_intensity = Philadelphia_june6_df['value'].mean()\n",
    "\n",
    "# Calculate sensor density for Detroit\n",
    "detroit_sensor_density = len(detroit_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Detroit\n",
    "detroit_pm25_intensity = detroit_june6_df['value'].mean()\n",
    "\n",
    "print(\"Sensor Density for Philadelphia International Airport:\", Philadelphia_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Philadelphia International Airport:\", Philadelphia_pm25_intensity)\n",
    "print(\"Sensor Density for Detroit:\", detroit_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Detroit:\", detroit_pm25_intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e1b7d-53b4-4d32-a7f0-06366e7b8e8e",
   "metadata": {},
   "source": [
    "\n",
    "Comment:\n",
    "Philadelphia International Airport's sensor density of 10 is remarkably lower compared to Detroit's extensive network of 5186 sensors. \n",
    "\n",
    "Despite this, Philadelphia experiences a significant intensity of PM2.5 pollution, recording 41.19 µg/m³ on June 6.\n",
    "In contrast, Detroit faces a higher intensity of PM2.5 pollution at 72.89 µg/m³, showcasing more severe air quality challenges.\n",
    "These disparities emphasize varying levels of air quality monitoring infrastructure and pollution levels between the two locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5faa3314-881b-43a4-908f-fea3e4d36184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Newark_mayjune_data\n"
     ]
    }
   ],
   "source": [
    "# Newark Liberty International Airport:  40.692501, -74.168701\n",
    "# Data from May to June\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time  # Import the time module\n",
    "\n",
    "url = \"https://api.openaq.org/v2/measurements?date_from=2023-05-01%2000%3A00%3A00&date_to=2023-06-30%2023%3A59%3A59&limit=35000&page=1&offset=0&sort=desc&parameter_id=2&coordinates=40.692501%2C-74.168701&radius=7500&order_by=datetime\"\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the response content (JSON) to a file\n",
    "    with open('Newark_mayjune_data.json', 'w') as json_file:\n",
    "        json.dump(response.json(), json_file, indent=4)\n",
    "    print(\"Data saved to Newark_mayjune_data\")\n",
    "else:\n",
    "    print(\"Error occurred while fetching data:\", response.text)\n",
    "\n",
    "time.sleep(2)  # Add a sleep time of 2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf568c2-92ef-4ef4-8824-e7887f36303e",
   "metadata": {},
   "source": [
    "Extracted data with date ranges from May to June and July to August and mereged those two into single json file named NEWARK_data.json. I am unable to extract all the data at once it is throwing error so i broke the daterange itno two different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5144eb18-1b2f-4e32-b16e-adb0d9b1d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Newark_Julyaug_data\n"
     ]
    }
   ],
   "source": [
    "# Newark Liberty International Airport:  40.692501, -74.168701\n",
    "# Data from July to August\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time  # Import the time module\n",
    "\n",
    "url = \"https://api.openaq.org/v2/measurements?date_from=2023-07-01%2000%3A00%3A00&date_to=2023-08-31%2023%3A59%3A59&limit=20000&page=1&offset=0&sort=desc&parameter_id=2&coordinates=40.692501%2C-74.168701&radius=7500&order_by=datetime\"\n",
    "\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the response content (JSON) to a file\n",
    "    with open('Newark_Julyaug_data.json', 'w') as json_file:\n",
    "        json.dump(response.json(), json_file, indent=4)\n",
    "    print(\"Data saved to Newark_Julyaug_data\")\n",
    "else:\n",
    "    print(\"Error occurred while fetching data:\", response.text)\n",
    "\n",
    "time.sleep(2)  # Add a sleep time of 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd7336c5-7431-4912-b4cb-a47a959677af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May-June data length: 35000\n",
      "July-August data length: 20000\n",
      "Combined JSON file 'NEWARK_data.json' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the content of the first JSON file (May-June data)\n",
    "with open('Newark_mayjune_data.json', 'r') as file:\n",
    "    mayjune_data = json.load(file)\n",
    "    print(\"May-June data length:\", len(mayjune_data[\"results\"]))\n",
    "\n",
    "# Read the content of the second JSON file (July-August data)\n",
    "with open('Newark_Julyaug_data.json', 'r') as file:\n",
    "    julyaug_data = json.load(file)\n",
    "    print(\"July-August data length:\", len(julyaug_data[\"results\"]))\n",
    "\n",
    "# Combine \"results\" data from both JSON files into a single list\n",
    "combined_results = mayjune_data[\"results\"] + julyaug_data[\"results\"]\n",
    "\n",
    "# Construct a new dictionary with combined \"results\"\n",
    "combined_data = {\n",
    "    \"meta\": {\n",
    "        \"source\": \"Newark_mayjune_data and Newark_Julyaug_data\"\n",
    "    },\n",
    "    \"results\": combined_results\n",
    "}\n",
    "\n",
    "# Write combined data to a JSON file\n",
    "with open('NEWARK_data.json', 'w') as file:\n",
    "    json.dump(combined_data, file)\n",
    "\n",
    "print(\"Combined JSON file 'NEWARK_data.json' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05def654-1eab-4e18-bf75-dc2348073efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in 'NEWARK_7.5km_data.csv' successfully.\n"
     ]
    }
   ],
   "source": [
    "####    Task: Transform, filter and store the OpenAQ data as CSV\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('NEWARK_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize the 'results' part of the JSON data\n",
    "results = data['results']\n",
    "df = pd.json_normalize(results)\n",
    "\n",
    "# Convert the date field to datetime64 if it exists\n",
    "if 'date.local' in df.columns:\n",
    "    df['local_time'] = pd.to_datetime(df['date.local'])\n",
    "\n",
    "# Check if the necessary columns are present in the DataFrame\n",
    "expected_columns = ['location', 'parameter', 'value', 'local_time']\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "# If any columns are missing, print a message and handle the issue accordingly\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    # Extract latitude and longitude from the nested 'coordinates' fields\n",
    "    df['sensor_lat'] = df['coordinates.latitude']\n",
    "    df['sensor_lon'] = df['coordinates.longitude']\n",
    "\n",
    "    # Reduce the DataFrame to include only specified fields\n",
    "    df = df[expected_columns + ['sensor_lat', 'sensor_lon']]\n",
    "\n",
    "    # Store it to a file called: 20230606_detroit_downtown_7_5km_aq.csv\n",
    "    filename = 'NEWARK_7.5km_data.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data stored in '{filename}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd13e061-9827-4ed1-a960-bf77c3d2f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Density for Newark International Airport: 453\n",
      "Intensity of PM2.5 for Newark International Airport: 54.55607064017661\n",
      "Sensor Density for Detroit: 5186\n",
      "Intensity of PM2.5 for Detroit: 72.8891245661396\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from both datasets into DataFrames\n",
    "Newark_df = pd.read_csv('NEWARK_7.5km_data.csv')\n",
    "detroit_df = pd.read_csv('20230606_detroit_downtown_7_5km_aq.csv')\n",
    "\n",
    "# Filter data for June 6, 2023\n",
    "Newark_june6_df = Newark_df.loc[Newark_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "detroit_june6_df = detroit_df.loc[detroit_df['local_time'].str.startswith(\"2023-06-06\")]\n",
    "\n",
    "# Calculate sensor density for Newark International Airport\n",
    "Newark_sensor_density = len(Newark_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Newark International Airport\n",
    "Newark_pm25_intensity = Newark_june6_df['value'].mean()\n",
    "\n",
    "# Calculate sensor density for Detroit\n",
    "detroit_sensor_density = len(detroit_june6_df)\n",
    "\n",
    "# Calculate intensity of PM2.5 for Detroit\n",
    "detroit_pm25_intensity = detroit_june6_df['value'].mean()\n",
    "\n",
    "print(\"Sensor Density for Newark International Airport:\", Newark_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Newark International Airport:\", Newark_pm25_intensity)\n",
    "print(\"Sensor Density for Detroit:\", detroit_sensor_density)\n",
    "print(\"Intensity of PM2.5 for Detroit:\", detroit_pm25_intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ccae7-d95b-4c29-9fc0-6480a7059855",
   "metadata": {},
   "source": [
    "\n",
    "Comment:\n",
    "\n",
    "On June 6, 2023, both Newark International Airport and Detroit had active air quality monitoring systems.\n",
    "However, Detroit has higher sensor density, 5186 sensors compared to Newark's 453.\n",
    "This stark difference suggests Detroit's more comprehensive coverage in monitoring air quality within its vicinity.\n",
    "Despite Newark's lower sensor density, it experienced a notable intensity of PM2.5 pollution, recording 54.56 µg/m³. Conversely, Detroit faced a significantly higher intensity of PM2.5 pollution at 72.89 µg/m³. \n",
    "These disparities underscore the varying levels of air quality monitoring infrastructure and pollution levels between the two locations, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe28e8e-6a29-4700-b8a3-2ce84a3ea4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sau24s]",
   "language": "python",
   "name": "conda-env-sau24s-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
